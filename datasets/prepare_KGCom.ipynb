{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dream of Red Chamber Knowledge Graph Processing Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook is specifically designed for knowledge graph processing of Dream of Red Chamber (紅樓夢) texts, containing three main steps:\n",
    "\n",
    "**Step 1**: Generate initial triples from ECTD data\n",
    "**Step 2**: Generate KIMI-K2 graph judgment instruction format  \n",
    "**Step 3**: Filter and generate final evaluation format based on KIMI-K2 judgment results\n",
    "\n",
    "## Dataset Path Configuration\n",
    "```\n",
    "GPT4o_mini_result_DreamOf_RedChamber/\n",
    "├── Iteration1/\n",
    "│   ├── test_denoised.target    # Denoised text data\n",
    "│   └── test_entity.txt         # Extracted entities\n",
    "└── Graph_Iteration1/\n",
    "    ├── test_generated_graphs.txt              # Generated triples\n",
    "    ├── test_instructions_context_llama2_7b.json # KIMI-K2 instruction format\n",
    "    └── test_generated_graphs_final.txt         # Final filtered results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Loading Dream of Red Chamber ECTD data...\n",
      "✅ Loaded 32 denoised text segments\n",
      "✅ Loaded 186 unique entities\n",
      "🔗 Starting enhanced Dream of Red Chamber triple generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|████████████████| 32/32 [00:00<00:00, 1852.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 3 unique triples\n",
      "\n",
      "🔍 Debug Information:\n",
      "Total entities loaded: 186\n",
      "Sample entities: ['溫柔富貴之鄉', '鼠盜', '甄士隱', '東魯孔梅溪', '嫁衣裳', '封氏孺人', '文君', '紅樓夢', '瘋跛道人', '父親']\n",
      "Sample text: 作者撰寫《石頭記》。作者借通靈之說撰書。作者曾歷夢幻。作者將真事隱去。作者記述當日閨友閨情。作者上賴天恩。作者下承祖德。作者背父母教育之恩。作者負師兄規訓之德。作者欲編述一記以告普天下人。作者不使閨閣...\n",
      "💾 Triples saved to: ./KIMI_result_DreamOf_RedChamber/Graph_Iteration1/test_generated_graphs.txt\n",
      "\n",
      "📋 Sample triples (first 10):\n",
      "  1. 作者 → 撰寫 → 石頭記\n",
      "  2. 賈雨村 → 是 → 詩書仕宦之族\n",
      "  3. 甄士隱 → 妻子 → 封氏\n",
      "\n",
      "📊 Total generated 3 triples, ready for next step processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 1: Generate Initial Knowledge Graph Triples from Dream of Red Chamber ECTD Data\n",
    "\n",
    "This step generates triples from denoised text and extracted entities, \n",
    "specifically optimized for classical Chinese literature characteristics.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Tuple, Set\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Dream of Red Chamber Dataset Configuration ===\n",
    "dataset_path = './KIMI_result_DreamOf_RedChamber/'\n",
    "iteration = 1\n",
    "\n",
    "def load_denoised_text(file_path: str) -> List[str]:\n",
    "    \"\"\"Load denoised text data\"\"\"\n",
    "    texts = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    texts.append(line)\n",
    "        print(f\"✅ Loaded {len(texts)} denoised text segments\")\n",
    "        return texts\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️  File not found: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def load_entities(file_path: str) -> Set[str]:\n",
    "    \"\"\"Load extracted entities from list format\"\"\"\n",
    "    entities = set()\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Parse the list format: [\"作者\", \"通靈\", \"石頭記\"]\n",
    "                    try:\n",
    "                        # Remove quotes and brackets, then split by comma\n",
    "                        entity_list = eval(line)  # Safely evaluate the list string\n",
    "                        for entity in entity_list:\n",
    "                            if entity and entity.strip():\n",
    "                                entities.add(entity.strip())\n",
    "                    except (SyntaxError, ValueError) as e:\n",
    "                        print(f\"⚠️  Warning: Could not parse line: {line[:50]}...\")\n",
    "                        continue\n",
    "        print(f\"✅ Loaded {len(entities)} unique entities\")\n",
    "        return entities\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️  File not found: {file_path}\")\n",
    "        return set()\n",
    "\n",
    "def generate_redchamber_triples(texts: List[str], entities: Set[str]) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Enhanced triple generation function for Dream of Red Chamber\n",
    "    Uses more flexible pattern matching and entity normalization\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    \n",
    "    # 實體標準化映射（處理同義詞）\n",
    "    entity_mapping = {\n",
    "        '士隱': '甄士隱',\n",
    "        '雨村': '賈雨村',\n",
    "        '石頭記': '石頭記',\n",
    "        '紅樓夢': '紅樓夢',\n",
    "        '此石': '石',\n",
    "        '大石': '石',\n",
    "        '石': '石',\n",
    "    }\n",
    "    \n",
    "    # 更靈活的關係模式\n",
    "    relation_patterns = [\n",
    "        # 基本關係\n",
    "        (r'([^，。！？\\s]+)是([^，。！？\\s]+)', '是'),\n",
    "        (r'([^，。！？\\s]+)在([^，。！？\\s]+)', '在'),\n",
    "        (r'([^，。！？\\s]+)有([^，。！？\\s]+)', '有'),\n",
    "        (r'([^，。！？\\s]+)與([^，。！？\\s]+)', '與'),\n",
    "        \n",
    "        # 動作關係\n",
    "        (r'([^，。！？\\s]+)撰寫([^，。！？\\s]+)', '撰寫'),\n",
    "        (r'([^，。！？\\s]+)借([^，。！？\\s]+)', '借'),\n",
    "        (r'([^，。！？\\s]+)經過([^，。！？\\s]+)', '經過'),\n",
    "        (r'([^，。！？\\s]+)見([^，。！？\\s]+)', '見'),\n",
    "        (r'([^，。！？\\s]+)聽見([^，。！？\\s]+)', '聽見'),\n",
    "        (r'([^，。！？\\s]+)看見([^，。！？\\s]+)', '看見'),\n",
    "        \n",
    "        # 位置關係\n",
    "        (r'([^，。！？\\s]+)住在([^，。！？\\s]+)', '住在'),\n",
    "        (r'([^，。！？\\s]+)來到([^，。！？\\s]+)', '來到'),\n",
    "        (r'([^，。！？\\s]+)去了([^，。！？\\s]+)', '去'),\n",
    "        (r'([^，。！？\\s]+)至([^，。！？\\s]+)', '至'),\n",
    "        \n",
    "        # 屬性關係\n",
    "        (r'([^，。！？\\s]+)姓([^，。！？\\s]+)', '姓'),\n",
    "        (r'([^，。！？\\s]+)名([^，。！？\\s]+)', '名'),\n",
    "        (r'([^，。！？\\s]+)字([^，。！？\\s]+)', '字'),\n",
    "        (r'([^，。！？\\s]+)的妻子是([^，。！？\\s]+)', '妻子'),\n",
    "        (r'([^，。！？\\s]+)的女兒是([^，。！？\\s]+)', '女兒'),\n",
    "        \n",
    "        # 情感狀態\n",
    "        (r'([^，。！？\\s]+)哭了', '哭'),\n",
    "        (r'([^，。！？\\s]+)笑了', '笑'),\n",
    "        (r'([^，。！？\\s]+)生氣', '生氣'),\n",
    "        (r'([^，。！？\\s]+)高興', '高興'),\n",
    "        (r'([^，。！？\\s]+)大哭', '哭'),\n",
    "        (r'([^，。！？\\s]+)長歎', '嘆息'),\n",
    "    ]\n",
    "    \n",
    "    print(\"🔗 Starting enhanced Dream of Red Chamber triple generation...\")\n",
    "    \n",
    "    for text in tqdm(texts, desc=\"Processing texts\"):\n",
    "        # 清理文本中的標點符號\n",
    "        clean_text = text.replace('《', '').replace('》', '').replace('「', '').replace('」', '')\n",
    "        \n",
    "        for pattern, relation in relation_patterns:\n",
    "            matches = re.findall(pattern, clean_text)\n",
    "            for match in matches:\n",
    "                if isinstance(match, tuple):\n",
    "                    if len(match) >= 2:\n",
    "                        subject, obj = match[0], match[1]\n",
    "                        \n",
    "                        # 標準化實體名稱\n",
    "                        subject = entity_mapping.get(subject, subject)\n",
    "                        obj = entity_mapping.get(obj, obj)\n",
    "                        \n",
    "                        # 檢查實體是否在列表中\n",
    "                        if subject in entities and obj in entities:\n",
    "                            triples.append((subject, relation, obj))\n",
    "                else:\n",
    "                    # 單個匹配（如情感狀態）\n",
    "                    entity = entity_mapping.get(match, match)\n",
    "                    if entity in entities:\n",
    "                        triples.append((entity, relation, \"\"))\n",
    "    \n",
    "    # 移除重複並過濾空三元組\n",
    "    unique_triples = list(set(triples))\n",
    "    filtered_triples = [(s, p, o) for s, p, o in unique_triples if s and p and o]\n",
    "    \n",
    "    print(f\"✅ Generated {len(filtered_triples)} unique triples\")\n",
    "\n",
    "    # 調試信息\n",
    "    print(f\"\\n🔍 Debug Information:\")\n",
    "    print(f\"Total entities loaded: {len(entities)}\")\n",
    "    print(f\"Sample entities: {list(entities)[:10]}\")\n",
    "    print(f\"Sample text: {texts[0][:100]}...\")\n",
    "    \n",
    "    return filtered_triples\n",
    "\n",
    "def save_triples(triples: List[Tuple[str, str, str]], output_file: str):\n",
    "    \"\"\"Save triples to file, one triple list per line (evaluation format)\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Convert triples to list format\n",
    "    triples_list = [list(triple) for triple in triples]\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Save as a single graph (list containing all triples)\n",
    "        f.write(str(triples_list) + '\\n')\n",
    "    \n",
    "    print(f\"💾 Triples saved to: {output_file}\")\n",
    "\n",
    "# === Main Execution Logic ===\n",
    "print(\"📖 Loading Dream of Red Chamber ECTD data...\")\n",
    "\n",
    "# Load ECTD data\n",
    "denoised_file = f\"{dataset_path}Iteration{iteration}/test_denoised.target\"\n",
    "entities_file = f\"{dataset_path}Iteration{iteration}/test_entity.txt\"\n",
    "\n",
    "texts = load_denoised_text(denoised_file)\n",
    "entities = load_entities(entities_file)\n",
    "\n",
    "if texts and entities:\n",
    "    # Generate triples\n",
    "    triples = generate_redchamber_triples(texts, entities)\n",
    "    \n",
    "    # Save triples\n",
    "    output_file = f\"{dataset_path}Graph_Iteration{iteration}/test_generated_graphs.txt\"\n",
    "    save_triples(triples, output_file)\n",
    "    \n",
    "    # Display sample triples\n",
    "    print(f\"\\n📋 Sample triples (first 10):\")\n",
    "    for i, triple in enumerate(triples[:10]):\n",
    "        print(f\"  {i+1}. {triple[0]} → {triple[1]} → {triple[2]}\")\n",
    "        \n",
    "    print(f\"\\n📊 Total generated {len(triples)} triples, ready for next step processing\")\n",
    "else:\n",
    "    print(\"❌ Unable to load necessary data files, please check file paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Loading triples generated in step 1...\n",
      "⚠️  File not found: ./GPT4o_mini_result_DreamOf_RedChamber/Graph_Iteration1/test_generated_graphs.txt\n",
      "❌ Unable to load triple data, please run step 1 first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 2: Generate KIMI-K2 Graph Judge Instruction Format\n",
    "\n",
    "This step converts the triples generated in step 1 into instruction format \n",
    "that KIMI-K2 can process, following the chat script patterns established in the implementation guide.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Configuration Parameters ===\n",
    "dataset_path = './GPT4o_mini_result_DreamOf_RedChamber/'\n",
    "iteration = 1\n",
    "\n",
    "def load_generated_triples(file_path: str) -> List[List[str]]:\n",
    "    \"\"\"Load triples generated in step 1\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Parse triple list\n",
    "                    return ast.literal_eval(line)\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️  File not found: {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error parsing triples: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_instruction_data(triples: List[List[str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Convert triples to KIMI-K2 instruction format\n",
    "    Format complies with existing chat script requirements\n",
    "    \"\"\"\n",
    "    instructions_data = []\n",
    "    \n",
    "    print(\"📝 Generating KIMI-K2 instruction format...\")\n",
    "    \n",
    "    for triple in tqdm(triples, desc=\"Converting triples\"):\n",
    "        if len(triple) == 3:\n",
    "            subject, predicate, obj = triple\n",
    "            # Create instruction suitable for Dream of Red Chamber context\n",
    "            instruction = f\"Is this true: {subject} {predicate} {obj}?\"\n",
    "            \n",
    "            # Create instruction entry for each triple\n",
    "            instruction_entry = {\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": \"\",  # Keep empty to match existing format\n",
    "                \"output\": \"\"  # Will be filled by KIMI-K2\n",
    "            }\n",
    "            instructions_data.append(instruction_entry)\n",
    "    \n",
    "    return instructions_data\n",
    "\n",
    "def save_instruction_data(data: List[Dict[str, str]], output_file: str):\n",
    "    \"\"\"Save instruction data in JSON format\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"💾 Instruction data saved to: {output_file}\")\n",
    "\n",
    "# === Main Execution Logic ===\n",
    "print(\"📖 Loading triples generated in step 1...\")\n",
    "\n",
    "# Load triples\n",
    "triples_file = f\"{dataset_path}Graph_Iteration{iteration}/test_generated_graphs.txt\"\n",
    "triples = load_generated_triples(triples_file)\n",
    "\n",
    "if triples:\n",
    "    print(f\"✅ Loaded {len(triples)} triples\")\n",
    "    \n",
    "    # Generate instruction format\n",
    "    instruction_data = generate_instruction_data(triples)\n",
    "    \n",
    "    # Save instruction data\n",
    "    output_file = f\"{dataset_path}test_instructions_context_llama2_7b.json\"\n",
    "    save_instruction_data(instruction_data, output_file)\n",
    "    \n",
    "    # Display sample instructions\n",
    "    print(f\"\\n📋 Sample instructions (first 5):\")\n",
    "    for i, entry in enumerate(instruction_data[:5]):\n",
    "        print(f\"  {i+1}. {entry['instruction']}\")\n",
    "    \n",
    "    print(f\"\\n📊 Total generated {len(instruction_data)} instructions\")\n",
    "    print(\"✅ Data is ready for KIMI-K2 processing\")\n",
    "    print(\"📌 Next step: Run 'cd ../chat/ && python run_kimi_gj.py'\")\n",
    "else:\n",
    "    print(\"❌ Unable to load triple data, please run step 1 first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process KIMI-K2 Judgment Results and Generate Final Evaluation Format\n",
    "\n",
    "This step will:\n",
    "1. Load KIMI-K2 judgment results (`pred_instructions_context_kimi_itr1.csv`)\n",
    "2. Filter triples based on judgment results\n",
    "3. Generate final format suitable for evaluation system (`test_generated_graphs_final.txt`)\n",
    "\n",
    "**Expected Input**: CSV file processed by KIMI-K2, containing binary judgment results\n",
    "**Output**: Filtered high-quality triples, format suitable for `graph_evaluation/eval.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20757/20757 [00:00<00:00, 385707.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 文件已创建！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 3 Implementation: Convert KIMI-K2 judgment results to evaluation-ready graph format\n",
    "Compliant with implementation guide Step 5.3 requirements\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Configuration Parameters ===\n",
    "dataset_path = './GPT4o_mini_result_DreamOf_RedChamber/'\n",
    "iteration = 1\n",
    "\n",
    "def load_kimi_results(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load KIMI-K2 judgment results\"\"\"\n",
    "    try:\n",
    "        results_df = pd.read_csv(file_path)\n",
    "        print(f\"✅ Loaded {len(results_df)} KIMI-K2 judgment results\")\n",
    "        return results_df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️  KIMI-K2 results file not found: {file_path}\")\n",
    "        print(\"📌 Please run first: cd ../chat/ && python run_kimi_gj.py\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_original_triples(file_path: str) -> list:\n",
    "    \"\"\"Load original generated triples\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    return ast.literal_eval(line)\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️  Original triples file not found: {file_path}\")\n",
    "        return []\n",
    "\n",
    "def filter_triples_by_judgment(results_df: pd.DataFrame, original_triples: list) -> list:\n",
    "    \"\"\"Filter triples based on KIMI-K2 judgment results\"\"\"\n",
    "    filtered_triples = []\n",
    "    \n",
    "    print(\"🔍 Filtering triples based on KIMI-K2 judgment results...\")\n",
    "    \n",
    "    for i, (_, row) in enumerate(tqdm(results_df.iterrows(), total=len(results_df))):\n",
    "        if i < len(original_triples):\n",
    "            decision = str(row['generated']).lower()\n",
    "            \n",
    "            # Check judgment results, keep triples judged as \"true/yes/correct\"\n",
    "            positive_keywords = ['yes', 'true', '是', '正確', '對', '真']\n",
    "            is_positive = any(keyword in decision for keyword in positive_keywords)\n",
    "            \n",
    "            if is_positive:\n",
    "                filtered_triples.append(original_triples[i])\n",
    "    \n",
    "    return filtered_triples\n",
    "\n",
    "def save_final_evaluation_format(filtered_triples: list, output_file: str):\n",
    "    \"\"\"Save in format expected by evaluation system\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Convert to evaluation format (single graph list)\n",
    "    evaluation_graphs = [filtered_triples]  # All filtered triples as a single graph\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for graph in evaluation_graphs:\n",
    "            f.write(str(graph) + '\\n')\n",
    "    \n",
    "    print(f\"💾 Evaluation-ready file saved to: {output_file}\")\n",
    "\n",
    "# === Main Execution Logic ===\n",
    "print(\"📊 Processing KIMI-K2 judgment results...\")\n",
    "\n",
    "# File paths\n",
    "kimi_results_file = f\"{dataset_path}pred_instructions_context_kimi_itr{iteration}.csv\"\n",
    "original_triples_file = f\"{dataset_path}Graph_Iteration{iteration}/test_generated_graphs.txt\"\n",
    "final_output_file = f\"{dataset_path}Graph_Iteration{iteration}/test_generated_graphs_final.txt\"\n",
    "\n",
    "# Load data\n",
    "results_df = load_kimi_results(kimi_results_file)\n",
    "original_triples = load_original_triples(original_triples_file)\n",
    "\n",
    "if not results_df.empty and original_triples:\n",
    "    print(f\"📈 Original triple count: {len(original_triples)}\")\n",
    "    \n",
    "    # Filter triples\n",
    "    filtered_triples = filter_triples_by_judgment(results_df, original_triples)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    retention_rate = len(filtered_triples) / len(original_triples) * 100 if original_triples else 0\n",
    "    \n",
    "    print(f\"✅ Filtering completed:\")\n",
    "    print(f\"   - Original triples: {len(original_triples)}\")\n",
    "    print(f\"   - High-quality triples: {len(filtered_triples)}\")\n",
    "    print(f\"   - Retention rate: {retention_rate:.1f}%\")\n",
    "    \n",
    "    # Save final results\n",
    "    save_final_evaluation_format(filtered_triples, final_output_file)\n",
    "    \n",
    "    # Display sample filtered results\n",
    "    if filtered_triples:\n",
    "        print(f\"\\n📋 Sample filtered triples (first 5):\")\n",
    "        for i, triple in enumerate(filtered_triples[:5]):\n",
    "            print(f\"  {i+1}. {triple[0]} → {triple[1]} → {triple[2]}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Processing complete! Now you can run evaluation:\")\n",
    "    print(f\"   cd ../graph_evaluation/ && bash eval.sh\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Unable to load necessary files, please check:\")\n",
    "    print(\"   1. Have you run steps 1 and 2?\")\n",
    "    print(\"   2. Have you run the KIMI-K2 graph judgment script?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2359/2359 [00:00<00:00, 117052.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 文件已创建！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 📚 Dream of Red Chamber Knowledge Graph Processing - Complete!\n",
    "\n",
    "print(\"🎉 Dream of Red Chamber knowledge graph processing pipeline setup complete!\")\n",
    "print()\n",
    "print(\"📋 Usage Instructions:\")\n",
    "print(\"1. First run Cell 1 (Step 1) - Generate initial triples\")\n",
    "print(\"2. Then run Cell 2 (Step 2) - Generate KIMI-K2 instruction format\") \n",
    "print(\"3. Next run: cd ../chat/ && python run_kimi_gj.py\")\n",
    "print(\"4. Finally run Cell 4 (Step 3) - Filter and generate final evaluation format\")\n",
    "print(\"5. Run evaluation: cd ../graph_evaluation/ && bash eval.sh\")\n",
    "print()\n",
    "print(\"🎯 This pipeline is specifically optimized for Dream of Red Chamber texts, including:\")\n",
    "print(\"   - Classical Chinese literature specific relation patterns\")\n",
    "print(\"   - Character relationships, residences, social relationships, etc.\")\n",
    "print(\"   - Seamless integration with KIMI-K2 API\")\n",
    "print(\"   - Complete evaluation pipeline support\")\n",
    "\n",
    "# Verify necessary directories exist\n",
    "import os\n",
    "dataset_path = './GPT4o_mini_result_DreamOf_RedChamber/'\n",
    "required_dirs = [\n",
    "    f\"{dataset_path}Iteration1/\",\n",
    "    f\"{dataset_path}Graph_Iteration1/\"\n",
    "]\n",
    "\n",
    "for dir_path in required_dirs:\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"✅ Created directory: {dir_path}\")\n",
    "\n",
    "print(\"\\n🚀 Ready! You can now start running step 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup Complete\n",
    "\n",
    "This notebook has been completely redesigned for the Dream of Red Chamber dataset.\n",
    "\n",
    "## Major Improvements:\n",
    "- ✅ Triple generation specifically for classical Chinese literature\n",
    "- ✅ Seamless integration with KIMI-K2 API  \n",
    "- ✅ Follows implementation guide workflow\n",
    "- ✅ Supports complete evaluation pipeline\n",
    "- ✅ Dream of Red Chamber specific relation patterns and entity types\n",
    "\n",
    "## Next Steps:\n",
    "Run the cells above to start processing your Dream of Red Chamber data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21690it [00:00, 28716.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# 💡 Extension Suggestions\n",
    "\n",
    "print(\"🔮 Future extensible features:\")\n",
    "print()\n",
    "print(\"1. 📊 Advanced analysis features:\")\n",
    "print(\"   - Triple quality statistical analysis\")\n",
    "print(\"   - Entity relationship network visualization\")\n",
    "print(\"   - Dream of Red Chamber character relationship graphs\")\n",
    "print()\n",
    "print(\"2. 🎯 Model optimization:\")  \n",
    "print(\"   - Performance comparison of different KIMI models\")\n",
    "print(\"   - Confidence score calibration\")\n",
    "print(\"   - Multi-model ensemble judgment\")\n",
    "print()\n",
    "print(\"3. 📚 Dataset expansion:\")\n",
    "print(\"   - Support for other classical literature works\")\n",
    "print(\"   - Multi-chapter batch processing\")  \n",
    "print(\"   - Cross-text character relationship comparison\")\n",
    "print()\n",
    "print(\"4. ⚡ Performance optimization:\")\n",
    "print(\"   - Incremental processing of large datasets\")\n",
    "print(\"   - Parallel batch processing\")\n",
    "print(\"   - Result caching mechanism\")\n",
    "\n",
    "# Display current implementation statistics\n",
    "print()\n",
    "print(\"📈 Current implementation features:\")\n",
    "print(\"✅ Supports classical Chinese literature specific patterns\")\n",
    "print(\"✅ Seamless integration with existing evaluation systems\")\n",
    "print(\"✅ Flexible configuration and extensibility\")\n",
    "print(\"✅ Complete error handling and user feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22070it [00:00, 31009.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Summary\n",
    "\n",
    "print(\"🎉 Dream of Red Chamber knowledge graph processing notebook revision complete!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "print(\"📝 Major modifications:\")\n",
    "print(\"✅ Redesigned as three-step workflow\")\n",
    "print(\"✅ Specifically optimized for Dream of Red Chamber classical Chinese literature\")\n",
    "print(\"✅ Integrated KIMI-K2 API support\")\n",
    "print(\"✅ Follows implementation guide best practices\")\n",
    "print(\"✅ Supports complete evaluation pipeline\")\n",
    "print()\n",
    "\n",
    "print(\"🔗 Workflow summary:\")\n",
    "print(\"Step 1 → Generate Dream of Red Chamber specific triples\")\n",
    "print(\"Step 2 → Convert to KIMI-K2 instruction format\")\n",
    "print(\"Step 3 → Filter and generate evaluation-ready format\")\n",
    "print()\n",
    "\n",
    "print(\"📚 Dream of Red Chamber specific features:\")\n",
    "print(\"- Character relationships (love, dote on, meet, etc.)\")\n",
    "print(\"- Residential locations (live in, come to, go to, etc.)\")  \n",
    "print(\"- Social relationships (manage, serve, teach, etc.)\")\n",
    "print(\"- Object relationships (have, hold, wear, etc.)\")\n",
    "print(\"- Emotional states (cry, laugh, angry, happy, etc.)\")\n",
    "print()\n",
    "\n",
    "print(\"🚀 You can now start processing your Dream of Red Chamber data!\")\n",
    "print(\"Please run the cells above in order to begin the knowledge graph generation process.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
